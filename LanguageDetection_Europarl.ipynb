{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The Europarl parallel corpus is extracted from the proceedings of the European Parliament. It includes versions in 21 European languages: Romanic (French, Italian, Spanish, Portuguese, Romanian), Germanic (English, Dutch, German, Danish, Swedish), Slavik (Bulgarian, Czech, Polish, Slovak, Slovene), Finni-Ugric (Finnish, Hungarian, Estonian), Baltic (Latvian, Lithuanian), and Greek.\n",
    "\n",
    "We will use this dataset for langauge detection of 21 languages used in dataset. It can thought of a multilingual text classification problem and we will use char level features for this task.\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "For each data unit, we have taken the following pre-processing steps:\n",
    "\n",
    " 1. Remove Tags <> and () brackets content\n",
    " 2. Split into multiple sentences using \\n split\n",
    " \n",
    "and then saved it to dataframe lang_df for further processing or modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Sentence to clean sentence or multiple sentences\n",
    "def clean_sentence(sent, multiple=1):\n",
    "    \n",
    "    # Remove () part from sent and <> tags\n",
    "    \n",
    "    sent = re.sub(\"\\([^)]*\\)\", '',sent)\n",
    "    sent = re.sub(\"<[^>]*>\", '', sent)\n",
    "    \n",
    "    # Split into multiple sentences if more than one sentence\n",
    "    \n",
    "    if multiple == 1:\n",
    "        return [x for x in sent.split('\\n') if x != '']\n",
    "    elif multiple == 0:\n",
    "        return sent\n",
    "\n",
    "# Preprocess whole folder of europarl data\n",
    "def preprocess_folder(input_org_data, preprocessing_text):\n",
    "    # Process each file by langauge folder take sentences into dataframe with label\n",
    "\n",
    "    if preprocessing_text == 0:\n",
    "        invalid_files = []\n",
    "\n",
    "        for i in output_labels:\n",
    "            print(\"Current Folder: \", i)\n",
    "            for j in glob.glob(input_org_data+'/'+i+'/*.txt'):\n",
    "                try:\n",
    "                    current_file = open(j).read()\n",
    "                    lang_df = lang_df.append(pd.DataFrame(list(zip(len(clean_sentence(current_file))*[i], clean_sentence(current_file))),\n",
    "                          columns=['label', 'sentence']))\n",
    "                except:\n",
    "                    invalid_files.append(j)\n",
    "                    print(j)\n",
    "                    pass\n",
    "\n",
    "        # Write to File\n",
    "        lang_df = lang_df.reset_index()\n",
    "        del lang_df['index']\n",
    "        lang_df.to_csv('lang_df.csv', index=False)\n",
    "\n",
    "    elif preprocessing_text == 1:\n",
    "        lang_df = pd.read_csv('lang_df.csv')\n",
    "    \n",
    "    else: # if dataset not required, features already generated\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "As we are using this dataset for language detection, we will choose char-level features, since this is multilinugal and we don't need much local domain information like sublevel classification or category classification for text, char-level features will be suited much better and can be used to create a unified vocablury with less diversification.\n",
    "\n",
    "We will use sklearn train_test_split to split into into training and test data for model validation, count_vectorizer with char analyzer for char level features(X) and label encoder for language type or (y). \n",
    "\n",
    "Count Vectorizer creates a char-level vocablury of the whole text data and then uses that to represent each sentence or unit of pre-processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "def extract_features(lang_df, feature_extraction_text):\n",
    "\n",
    "    if feature_extraction_text == 0:\n",
    "        # Split into training and validation dataset\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(list(lang_df.sentence), list(lang_df.label), test_size=0.33, random_state=42)\n",
    "\n",
    "        # Generate character level features for X and Label Encoder for y\n",
    "\n",
    "        # Train and Test\n",
    "\n",
    "        # Sentence X\n",
    "        count_vectorizer = CountVectorizer(analyzer='char')\n",
    "        X_train_features = count_vectorizer.fit_transform(X_train)\n",
    "        X_test_features = count_vectorizer.transform(X_test)\n",
    "\n",
    "        # Label Y\n",
    "        label_encoder = preprocessing.LabelEncoder()\n",
    "        y_train_features = label_encoder.fit_transform(y_train)\n",
    "        y_test_features = label_encoder.transform(y_test)\n",
    "\n",
    "        # Pickle Save Features\n",
    "\n",
    "        # Saving the features\n",
    "        with open('features.pkl', 'wb') as f:  \n",
    "            pickle.dump([X_train_features, X_test_features, y_train_features, y_test_features, count_vectorizer, label_encoder], f)\n",
    "\n",
    "    elif feature_extraction_text == 1:\n",
    "        # Restoring features and variables\n",
    "        with open('X_train_features.pkl', 'rb') as f:\n",
    "            X_train_features = pickle.load(f)[0]\n",
    "\n",
    "        with open('y_train_features.pkl', 'rb') as f:\n",
    "            y_train_features = pickle.load(f)[0]\n",
    "\n",
    "        with open('X_test_features.pkl', 'rb') as f:\n",
    "            X_test_features = pickle.load(f)[0]\n",
    "\n",
    "        with open('y_test_features.pkl', 'rb') as f:\n",
    "            y_test_features = pickle.load(f)[0]\n",
    "\n",
    "        with open('count_vectorizer_label_encoder.pkl', 'rb') as f:\n",
    "            count_vectorizer, label_encoder = pickle.load(f)\n",
    "        \n",
    "    return X_train_features, X_test_features, y_train_features, y_test_features, count_vectorizer, label_encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-1 Sklearn Multinomial Naive Bayes\n",
    "\n",
    "Multinomial Naive Bayes classifier is suitable for classification with discrete features, is well suited for text features. \n",
    "\n",
    "We will use the training features generated to fit to the model, no hypertuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Evaluation\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Train and test on MNB, input only features\n",
    "def run_mnb(X, y, clf=None):\n",
    "    \n",
    "    # Train mode, train and return model\n",
    "    if clf == None:\n",
    "        \n",
    "        X_train_features = X\n",
    "        y_train_features = y\n",
    "        \n",
    "        clf = MultinomialNB()\n",
    "\n",
    "        clf.fit(X_train_features, y_train_features)\n",
    "        \n",
    "        return clf\n",
    "    \n",
    "    else: # Test mode return F1 Accuracy\n",
    "        \n",
    "        X_test_features = X\n",
    "        y_test_features = y\n",
    "        \n",
    "        return f1_score(y_test_features, clf.predict(X_test_features), average='macro')  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline\n",
    "\n",
    "Using pre-processing funtions to pre-process data, and then generating features, training the model and testing it on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Model Pipeline\n",
    "\n",
    "# Original Data\n",
    "input_org_data = 'txt' # Path of data folder\n",
    "output_labels = os.listdir(input_org_data) # Labels for Languages in Dataset\n",
    "\n",
    "# Dataframe for processed data, columns: label, sentence\n",
    "lang_df = pd.DataFrame(columns=['label', 'sentence'])\n",
    "\n",
    "# Config and Execute\n",
    "\n",
    "# Preprocessing\n",
    "preprocessing_text = 2\n",
    "\n",
    "lang_df = preprocess_folder(input_org_data, preprocessing_text)\n",
    "\n",
    "if lang_df == True:\n",
    "    feature_extraction_text = 1\n",
    "else:\n",
    "    feature_extraction_text = 0\n",
    "\n",
    "# Feature Extraction\n",
    "X_train_features, X_test_features, y_train_features, y_test_features, count_vectorizer, label_encoder = extract_features(lang_df, feature_extraction_text)\n",
    "\n",
    "# Model\n",
    "\n",
    "# Train\n",
    "mnb_clf = run_mnb(X_train_features, y_train_features)\n",
    "\n",
    "# Test\n",
    "\n",
    "#f1_test = run_mnb(X_test_features, y_test_features, mnb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Bi-RNN Classification\n",
    "\n",
    "Using Bi-RNN with LSTM Cell for language detection.\n",
    "\n",
    "### Model Information\n",
    "    * 75x2 units LSTM cell bi-directional with concatination of outputs\n",
    "    * Softmax Activation\n",
    "    * Crossentropy Loss\n",
    "    * Adam Optimizer\n",
    "    * Mannual Batch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Keras Bi-RNN Classification\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM, Flatten, Bidirectional\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def run_keras_birnn(X, y, model_path=None):\n",
    "    \n",
    "    if model_path == None: # Train\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(75), input_shape=(1, X.shape[1]), merge_mode='concat'))\n",
    "        #model.add(LSTM(75, input_shape=(1, X_train_features.shape[1])))\n",
    "        model.add(Dense(len(output_labels), activation='softmax')) \n",
    "        model.summary()\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        start_idx = 0\n",
    "        idx_step = 100000\n",
    "\n",
    "        X_train_features, y_train_features = shuffle(X, y, random_state=0)\n",
    "\n",
    "        for i in range(idx_step, X_train_features.shape[0], idx_step):\n",
    "            #print(X_train_features[start_idx:i].toarray().reshape(X_train_features[start_idx:i].shape[0], 1, X_train_features[start_idx:i].shape[1]).shape)\n",
    "            model_ins = model.fit(X_train_features[start_idx:i].toarray().reshape(X_train_features[start_idx:i].shape[0], 1, X_train_features[start_idx:i].shape[1]), to_categorical(y_train_features[start_idx:i]), \n",
    "                                  epochs=2,\n",
    "                                 verbose=1,\n",
    "                                 validation_split=0.1)\n",
    "            start_idx = i\n",
    "\n",
    "        # Save Keras Model\n",
    "        model.save('keras_model.h5')\n",
    "        \n",
    "    else: # Test and evaluate\n",
    "        model = load_model(model_path)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Bi-RNN Classification\n",
    "\n",
    "### Model Information\n",
    "    * 75x2 units LSTM cell bi-directional with concatination of outputs\n",
    "    * Softmax Activation\n",
    "    * Crossentropy Loss\n",
    "    * Adam Optimizer\n",
    "    * Mannual Batch Training\n",
    "    \n",
    "### Model Params\n",
    "    * learning_rate = 0.01\n",
    "    * n_epoch = 10\n",
    "\n",
    "### Layer Params\n",
    "    * vocab_size = 322\n",
    "    * num_classes = 21\n",
    "    * hidden_dim = 75\n",
    "    * timesteps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF BiRNN Classification\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from keras.utils import to_categorical\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# Model Params\n",
    "learning_rate = 0.01\n",
    "n_epoch = 10\n",
    "\n",
    "# Layer Params\n",
    "vocab_size = X_train_features.shape[1]\n",
    "num_classes = len(output_labels)\n",
    "hidden_dim = 75\n",
    "timesteps = 1\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(tf.float32, shape=[None, timesteps, vocab_size])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
    "\n",
    "# Weights\n",
    "W_h = tf.Variable(tf.random_normal([2*hidden_dim, num_classes]))\n",
    "C_h = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "# Bi-LSTM Cell\n",
    "with tf.name_scope(\"BiLSTM\"):\n",
    "    with tf.variable_scope('forward'):\n",
    "        lstm_fw = tf.nn.rnn_cell.LSTMCell(hidden_dim, forget_bias=1.0, state_is_tuple=True)\n",
    "    with tf.variable_scope('backward'):\n",
    "        lstm_bw = tf.nn.rnn_cell.LSTMCell(hidden_dim, forget_bias=1.0, state_is_tuple=True)\n",
    "    \n",
    "    (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw,\n",
    "                                                                     cell_bw=lstm_bw,\n",
    "                                                                     inputs=X,\n",
    "                                                                     dtype=tf.float32,\n",
    "                                                                     scope=\"BiLSTM\")\n",
    "\n",
    "\n",
    "\n",
    "outputs = tf.concat([output_fw, output_bw], axis=2)\n",
    "outputs_flat = tf.reshape(outputs, [-1, 2 * hidden_dim])\n",
    "logits = tf.matmul(outputs_flat, W_h) + C_h\n",
    "#scores = tf.reshape(pred, [-1, batch_seq_len, num_classes])\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=Y))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Batch  1.0  -------------\n",
      "Epoch 1,  Loss= 4.8993, Training Accuracy= 0.228\n",
      "Epoch 5,  Loss= 1.7260, Training Accuracy= 0.597\n",
      "Epoch 10,  Loss= 0.4982, Training Accuracy= 0.867\n",
      "Test Accuracy: 0.86764\n",
      "---------- Batch  2.0  -------------\n",
      "Epoch 1,  Loss= 0.3548, Training Accuracy= 0.916\n",
      "Epoch 5,  Loss= 0.1911, Training Accuracy= 0.959\n",
      "Epoch 10,  Loss= 0.1213, Training Accuracy= 0.970\n",
      "Test Accuracy: 0.96967\n",
      "---------- Batch  3.0  -------------\n",
      "Epoch 1,  Loss= 0.1149, Training Accuracy= 0.972\n",
      "Epoch 5,  Loss= 0.0968, Training Accuracy= 0.975\n",
      "Epoch 10,  Loss= 0.0829, Training Accuracy= 0.978\n",
      "Test Accuracy: 0.97661\n",
      "---------- Batch  4.0  -------------\n",
      "Epoch 1,  Loss= 0.0848, Training Accuracy= 0.978\n",
      "Epoch 5,  Loss= 0.0783, Training Accuracy= 0.979\n",
      "Epoch 10,  Loss= 0.0711, Training Accuracy= 0.981\n",
      "Test Accuracy: 0.98006\n",
      "---------- Batch  5.0  -------------\n",
      "Epoch 1,  Loss= 0.0707, Training Accuracy= 0.981\n",
      "Epoch 5,  Loss= 0.0664, Training Accuracy= 0.981\n",
      "Epoch 10,  Loss= 0.0622, Training Accuracy= 0.982\n",
      "Test Accuracy: 0.98166\n",
      "---------- Batch  6.0  -------------\n",
      "Epoch 1,  Loss= 0.0667, Training Accuracy= 0.981\n",
      "Epoch 5,  Loss= 0.0634, Training Accuracy= 0.982\n",
      "Epoch 10,  Loss= 0.0597, Training Accuracy= 0.983\n",
      "Test Accuracy: 0.98323\n",
      "---------- Batch  7.0  -------------\n",
      "Epoch 1,  Loss= 0.0569, Training Accuracy= 0.984\n",
      "Epoch 5,  Loss= 0.0548, Training Accuracy= 0.985\n",
      "Epoch 10,  Loss= 0.0519, Training Accuracy= 0.985\n",
      "Test Accuracy: 0.98398\n",
      "---------- Batch  8.0  -------------\n",
      "Epoch 1,  Loss= 0.0563, Training Accuracy= 0.984\n",
      "Epoch 5,  Loss= 0.0545, Training Accuracy= 0.984\n",
      "Epoch 10,  Loss= 0.0518, Training Accuracy= 0.985\n",
      "Test Accuracy: 0.98464\n",
      "---------- Batch  9.0  -------------\n",
      "Epoch 1,  Loss= 0.0531, Training Accuracy= 0.985\n",
      "Epoch 5,  Loss= 0.0514, Training Accuracy= 0.985\n",
      "Epoch 10,  Loss= 0.0492, Training Accuracy= 0.986\n",
      "Test Accuracy: 0.98536\n",
      "---------- Batch  10.0  -------------\n",
      "Epoch 1,  Loss= 0.0512, Training Accuracy= 0.985\n",
      "Epoch 5,  Loss= 0.0497, Training Accuracy= 0.986\n",
      "Epoch 10,  Loss= 0.0477, Training Accuracy= 0.986\n",
      "Test Accuracy: 0.98599\n",
      "---------- Batch  11.0  -------------\n",
      "Epoch 1,  Loss= 0.0463, Training Accuracy= 0.987\n",
      "Epoch 5,  Loss= 0.0447, Training Accuracy= 0.987\n",
      "Epoch 10,  Loss= 0.0425, Training Accuracy= 0.988\n",
      "Test Accuracy: 0.98629\n",
      "---------- Batch  12.0  -------------\n",
      "Epoch 1,  Loss= 0.0475, Training Accuracy= 0.986\n",
      "Epoch 5,  Loss= 0.0461, Training Accuracy= 0.987\n",
      "Epoch 10,  Loss= 0.0441, Training Accuracy= 0.987\n",
      "Test Accuracy: 0.98649\n",
      "---------- Batch  13.0  -------------\n",
      "Epoch 1,  Loss= 0.0472, Training Accuracy= 0.987\n",
      "Epoch 5,  Loss= 0.0458, Training Accuracy= 0.987\n",
      "Epoch 10,  Loss= 0.0440, Training Accuracy= 0.988\n",
      "Test Accuracy: 0.98696\n",
      "---------- Batch  14.0  -------------\n",
      "Epoch 1,  Loss= 0.0449, Training Accuracy= 0.987\n",
      "Epoch 5,  Loss= 0.0436, Training Accuracy= 0.988\n",
      "Epoch 10,  Loss= 0.0418, Training Accuracy= 0.988\n",
      "Test Accuracy: 0.98692\n",
      "---------- Batch  15.0  -------------\n",
      "Epoch 1,  Loss= 0.0444, Training Accuracy= 0.987\n",
      "Epoch 5,  Loss= 0.0430, Training Accuracy= 0.987\n",
      "Epoch 10,  Loss= 0.0412, Training Accuracy= 0.988\n",
      "Test Accuracy: 0.98731\n",
      "---------- Batch  16.0  -------------\n",
      "Epoch 1,  Loss= 0.0454, Training Accuracy= 0.987\n",
      "Epoch 5,  Loss= 0.0439, Training Accuracy= 0.987\n",
      "Epoch 10,  Loss= 0.0418, Training Accuracy= 0.988\n",
      "Test Accuracy: 0.98735\n",
      "---------- Batch  17.0  -------------\n",
      "Epoch 1,  Loss= 0.0446, Training Accuracy= 0.987\n",
      "Epoch 5,  Loss= 0.0432, Training Accuracy= 0.987\n",
      "Epoch 10,  Loss= 0.0413, Training Accuracy= 0.988\n",
      "Test Accuracy: 0.98774\n",
      "---------- Batch  18.0  -------------\n",
      "Epoch 1,  Loss= 0.0426, Training Accuracy= 0.988\n",
      "Epoch 5,  Loss= 0.0413, Training Accuracy= 0.988\n",
      "Epoch 10,  Loss= 0.0393, Training Accuracy= 0.989\n",
      "Test Accuracy: 0.98786\n",
      "---------- Batch  19.0  -------------\n",
      "Epoch 1,  Loss= 0.0440, Training Accuracy= 0.988\n",
      "Epoch 5,  Loss= 0.0426, Training Accuracy= 0.988\n",
      "Epoch 10,  Loss= 0.0407, Training Accuracy= 0.989\n",
      "Test Accuracy: 0.98781\n",
      "---------- Batch  20.0  -------------\n",
      "Epoch 1,  Loss= 0.0423, Training Accuracy= 0.988\n",
      "Epoch 5,  Loss= 0.0410, Training Accuracy= 0.988\n",
      "Epoch 10,  Loss= 0.0394, Training Accuracy= 0.989\n",
      "Test Accuracy: 0.98796\n",
      "---------- Batch  21.0  -------------\n",
      "Epoch 1,  Loss= 0.0414, Training Accuracy= 0.988\n",
      "Epoch 5,  Loss= 0.0400, Training Accuracy= 0.989\n",
      "Epoch 10,  Loss= 0.0382, Training Accuracy= 0.989\n",
      "Test Accuracy: 0.98828\n",
      "---------- Batch  22.0  -------------\n",
      "Epoch 1,  Loss= 0.0404, Training Accuracy= 0.988\n",
      "Epoch 5,  Loss= 0.0392, Training Accuracy= 0.989\n",
      "Epoch 10,  Loss= 0.0373, Training Accuracy= 0.989\n",
      "Test Accuracy: 0.98854\n",
      "---------- Batch  23.0  -------------\n",
      "Epoch 1,  Loss= 0.0427, Training Accuracy= 0.988\n",
      "Epoch 5,  Loss= 0.0413, Training Accuracy= 0.988\n",
      "Epoch 10,  Loss= 0.0393, Training Accuracy= 0.989\n",
      "Test Accuracy: 0.98863\n",
      "---------- Batch  24.0  -------------\n",
      "Epoch 1,  Loss= 0.0409, Training Accuracy= 0.988\n",
      "Epoch 5,  Loss= 0.0393, Training Accuracy= 0.989\n",
      "Epoch 10,  Loss= 0.0373, Training Accuracy= 0.989\n",
      "Test Accuracy: 0.98864\n",
      "---------- Batch  25.0  -------------\n",
      "Epoch 1,  Loss= 0.0403, Training Accuracy= 0.989\n",
      "Epoch 5,  Loss= 0.0389, Training Accuracy= 0.989\n",
      "Epoch 10,  Loss= 0.0370, Training Accuracy= 0.989\n",
      "Test Accuracy: 0.98857\n",
      "---------- Batch  26.0  -------------\n",
      "Epoch 1,  Loss= 0.0399, Training Accuracy= 0.989\n",
      "Epoch 5,  Loss= 0.0386, Training Accuracy= 0.989\n",
      "Epoch 10,  Loss= 0.0367, Training Accuracy= 0.990\n",
      "Test Accuracy: 0.98863\n",
      "---------- Batch  27.0  -------------\n",
      "Epoch 1,  Loss= 0.0408, Training Accuracy= 0.988\n",
      "Epoch 5,  Loss= 0.0393, Training Accuracy= 0.988\n",
      "Epoch 10,  Loss= 0.0372, Training Accuracy= 0.989\n",
      "Test Accuracy: 0.98902\n",
      "---------- Batch  28.0  -------------\n",
      "Epoch 1,  Loss= 0.0394, Training Accuracy= 0.988\n",
      "Epoch 5,  Loss= 0.0382, Training Accuracy= 0.989\n",
      "Epoch 10,  Loss= 0.0364, Training Accuracy= 0.989\n",
      "Test Accuracy: 0.98906\n",
      "---------- Batch  29.0  -------------\n",
      "Epoch 1,  Loss= 0.0397, Training Accuracy= 0.989\n",
      "Epoch 5,  Loss= 0.0382, Training Accuracy= 0.989\n",
      "Epoch 10,  Loss= 0.0361, Training Accuracy= 0.990\n",
      "Test Accuracy: 0.98911\n",
      "---------- Batch  30.0  -------------\n",
      "Epoch 1,  Loss= 0.0400, Training Accuracy= 0.989\n",
      "Epoch 5,  Loss= 0.0386, Training Accuracy= 0.989\n",
      "Epoch 10,  Loss= 0.0368, Training Accuracy= 0.989\n",
      "Test Accuracy: 0.98928\n",
      "---------- Batch  31.0  -------------\n",
      "Epoch 1,  Loss= 0.0389, Training Accuracy= 0.989\n",
      "Epoch 5,  Loss= 0.0373, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0355, Training Accuracy= 0.990\n",
      "Test Accuracy: 0.98936\n",
      "---------- Batch  32.0  -------------\n",
      "Epoch 1,  Loss= 0.0394, Training Accuracy= 0.989\n",
      "Epoch 5,  Loss= 0.0381, Training Accuracy= 0.989\n",
      "Epoch 10,  Loss= 0.0360, Training Accuracy= 0.989\n",
      "Test Accuracy: 0.98929\n",
      "---------- Batch  33.0  -------------\n",
      "Epoch 1,  Loss= 0.0374, Training Accuracy= 0.989\n",
      "Epoch 5,  Loss= 0.0359, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0338, Training Accuracy= 0.990\n",
      "Test Accuracy: 0.98942\n",
      "---------- Batch  34.0  -------------\n",
      "Epoch 1,  Loss= 0.0396, Training Accuracy= 0.989\n",
      "Epoch 5,  Loss= 0.0381, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0361, Training Accuracy= 0.990\n",
      "Test Accuracy: 0.98958\n",
      "---------- Batch  35.0  -------------\n",
      "Epoch 1,  Loss= 0.0371, Training Accuracy= 0.989\n",
      "Epoch 5,  Loss= 0.0358, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0336, Training Accuracy= 0.990\n",
      "Test Accuracy: 0.98949\n",
      "---------- Batch  36.0  -------------\n",
      "Epoch 1,  Loss= 0.0386, Training Accuracy= 0.989\n",
      "Epoch 5,  Loss= 0.0371, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0352, Training Accuracy= 0.990\n",
      "Test Accuracy: 0.9893\n",
      "---------- Batch  37.0  -------------\n",
      "Epoch 1,  Loss= 0.0364, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0348, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0332, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.98951\n",
      "---------- Batch  38.0  -------------\n",
      "Epoch 1,  Loss= 0.0365, Training Accuracy= 0.989\n",
      "Epoch 5,  Loss= 0.0350, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0332, Training Accuracy= 0.990\n",
      "Test Accuracy: 0.98976\n",
      "---------- Batch  39.0  -------------\n",
      "Epoch 1,  Loss= 0.0375, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0359, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0338, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.98982\n",
      "---------- Batch  40.0  -------------\n",
      "Epoch 1,  Loss= 0.0357, Training Accuracy= 0.990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5,  Loss= 0.0341, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0320, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.98961\n",
      "---------- Batch  41.0  -------------\n",
      "Epoch 1,  Loss= 0.0375, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0360, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0339, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.98987\n",
      "---------- Batch  42.0  -------------\n",
      "Epoch 1,  Loss= 0.0374, Training Accuracy= 0.989\n",
      "Epoch 5,  Loss= 0.0361, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0341, Training Accuracy= 0.990\n",
      "Test Accuracy: 0.9898\n",
      "---------- Batch  43.0  -------------\n",
      "Epoch 1,  Loss= 0.0368, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0353, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0332, Training Accuracy= 0.990\n",
      "Test Accuracy: 0.98985\n",
      "---------- Batch  44.0  -------------\n",
      "Epoch 1,  Loss= 0.0367, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0350, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0330, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.99018\n",
      "---------- Batch  45.0  -------------\n",
      "Epoch 1,  Loss= 0.0353, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0339, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0319, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.98999\n",
      "---------- Batch  46.0  -------------\n",
      "Epoch 1,  Loss= 0.0366, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0353, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0334, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.98992\n",
      "---------- Batch  47.0  -------------\n",
      "Epoch 1,  Loss= 0.0351, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0334, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0312, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.99013\n",
      "---------- Batch  48.0  -------------\n",
      "Epoch 1,  Loss= 0.0348, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0337, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0318, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.99003\n",
      "---------- Batch  49.0  -------------\n",
      "Epoch 1,  Loss= 0.0342, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0327, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0307, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.9898\n",
      "---------- Batch  50.0  -------------\n",
      "Epoch 1,  Loss= 0.0343, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0328, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0307, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.99012\n",
      "---------- Batch  51.0  -------------\n",
      "Epoch 1,  Loss= 0.0344, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0328, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0307, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.9905\n",
      "---------- Batch  52.0  -------------\n",
      "Epoch 1,  Loss= 0.0323, Training Accuracy= 0.991\n",
      "Epoch 5,  Loss= 0.0310, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0292, Training Accuracy= 0.992\n",
      "Test Accuracy: 0.9902\n",
      "---------- Batch  53.0  -------------\n",
      "Epoch 1,  Loss= 0.0354, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0337, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0318, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.99053\n",
      "---------- Batch  54.0  -------------\n",
      "Epoch 1,  Loss= 0.0357, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0341, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0319, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.99028\n",
      "---------- Batch  55.0  -------------\n",
      "Epoch 1,  Loss= 0.0358, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0341, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0319, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.9902\n",
      "---------- Batch  56.0  -------------\n",
      "Epoch 1,  Loss= 0.0360, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0342, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0321, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.99004\n",
      "---------- Batch  57.0  -------------\n",
      "Epoch 1,  Loss= 0.0359, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0340, Training Accuracy= 0.990\n",
      "Epoch 10,  Loss= 0.0318, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.99032\n",
      "---------- Batch  58.0  -------------\n",
      "Epoch 1,  Loss= 0.0346, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0331, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0310, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.9903\n",
      "---------- Batch  59.0  -------------\n",
      "Epoch 1,  Loss= 0.0346, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0328, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0310, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.99041\n",
      "---------- Batch  60.0  -------------\n",
      "Epoch 1,  Loss= 0.0344, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0328, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0306, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.99042\n",
      "---------- Batch  61.0  -------------\n",
      "Epoch 1,  Loss= 0.0327, Training Accuracy= 0.991\n",
      "Epoch 5,  Loss= 0.0313, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0294, Training Accuracy= 0.992\n",
      "Test Accuracy: 0.99053\n",
      "---------- Batch  62.0  -------------\n",
      "Epoch 1,  Loss= 0.0328, Training Accuracy= 0.991\n",
      "Epoch 5,  Loss= 0.0311, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0290, Training Accuracy= 0.992\n",
      "Test Accuracy: 0.99044\n",
      "---------- Batch  63.0  -------------\n",
      "Epoch 1,  Loss= 0.0349, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0332, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0308, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.99056\n",
      "---------- Batch  64.0  -------------\n",
      "Epoch 1,  Loss= 0.0349, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0334, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0314, Training Accuracy= 0.991\n",
      "Test Accuracy: 0.9906\n",
      "---------- Batch  65.0  -------------\n",
      "Epoch 1,  Loss= 0.0339, Training Accuracy= 0.990\n",
      "Epoch 5,  Loss= 0.0320, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0298, Training Accuracy= 0.992\n",
      "Test Accuracy: 0.99049\n",
      "---------- Batch  66.0  -------------\n",
      "Epoch 1,  Loss= 0.0336, Training Accuracy= 0.991\n",
      "Epoch 5,  Loss= 0.0320, Training Accuracy= 0.991\n",
      "Epoch 10,  Loss= 0.0299, Training Accuracy= 0.992\n",
      "Test Accuracy: 0.99056\n"
     ]
    }
   ],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "X_train_features, y_train_features = shuffle(X_train_features, y_train_features, random_state=0)\n",
    "test_X, test_Y = shuffle(X_test_features, y_test_features, random_state=0)\n",
    "test_X, test_Y = test_X[0:100000], test_Y[0:100000]\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #saver.restore(sess, \"./model_tf/model_tf\")\n",
    "    \n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Batch Vars\n",
    "    start_idx = 0\n",
    "    idx_step = 100000\n",
    "\n",
    "    # Batch Loop\n",
    "\n",
    "    for i in range(idx_step, X_train_features.shape[0], idx_step):\n",
    "        \n",
    "        #if (i/idx_step >= 506):\n",
    "        \n",
    "        print(\"---------- Batch \", i/idx_step, \" -------------\")\n",
    "\n",
    "        batch_x, batch_y = shuffle( X_train_features[start_idx:i].toarray().reshape(X_train_features[start_idx:i].shape[0], 1, X_train_features[start_idx:i].shape[1]), to_categorical(y_train_features[start_idx:i]), random_state=0)\n",
    "\n",
    "        # Epoch Loop\n",
    "        for step in range(1, n_epoch+1):\n",
    "\n",
    "            # Reshuffle each epoch\n",
    "            batch_x, batch_y = shuffle(batch_x, batch_y, random_state=0)\n",
    "\n",
    "            # Feed Batch\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "            if step % 5 == 0 or step == 1:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                     Y: batch_y})\n",
    "                print(\"Epoch \" + str(step) + \",  Loss= \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.3f}\".format(acc))\n",
    "        \n",
    "        # Move Index\n",
    "        start_idx = i\n",
    "        \n",
    "        # Mem Fix\n",
    "        del batch_x\n",
    "        del batch_y\n",
    "\n",
    "        # Test Accuracy\n",
    "        print(\"Test Accuracy:\", \\\n",
    "            sess.run(accuracy, feed_dict={X: test_X.toarray().reshape(test_X.shape[0], 1, test_X.shape[1]), Y: to_categorical(test_Y)}))\n",
    "\n",
    "        saver.save(sess, './model_tf/model_tf_adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation of all the three models on Fellowship.ai custom dataset.\n",
    "\n",
    "Evaluation is carried out on accuracy score metric, which is standard classification accuracy metric in all the models.\n",
    "\n",
    "### Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20828/20828 [==============================] - 3s 147us/step\n",
      "MNB(Sklearn) ACC:  0.9704724409448819 , Keras BI-RNN ACC:  0.9718167850969849\n",
      "INFO:tensorflow:Restoring parameters from ./model_tf/model_tf_adam\n",
      "TF BI-RNN Accuracy: 0.9703284\n"
     ]
    }
   ],
   "source": [
    "# Custom Test File, Fellowship.AI\n",
    "\n",
    "test_fs = pd.read_csv('europarl.test', sep=\"\\t\", header=None)\n",
    "\n",
    "test_fs[1] = [clean_sentence(i, 0) for i in test_fs[1]]\n",
    "\n",
    "# Sklearn Evaluate\n",
    "mb_acc = accuracy_score(label_encoder.transform(test_fs[0]), mnb_clf.predict(count_vectorizer.transform(test_fs[1])))\n",
    "\n",
    "# Keras Evaluate\n",
    "test_fs_X = count_vectorizer.transform(test_fs[1]).toarray()\n",
    "keras_acc = run_keras_birnn(None, None, model_path='/media/abhishek/55840FA23CCD9EE9/FellowshipAI_Challenge/keras_model_2epoch.h5').evaluate(test_fs_X.reshape(test_fs_X.shape[0], 1, test_fs_X.shape[1]), to_categorical(label_encoder.transform(test_fs[0])))\n",
    "\n",
    "print(\"MNB(Sklearn) Accuracy: \", mb_acc, \", Keras BI-RNN Accuracy: \", keras_acc[1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model_tf/model_tf_adam\")\n",
    "    \n",
    "    print(\"TF BI-RNN Accuracy:\", \\\n",
    "            sess.run(accuracy, feed_dict={X: test_fs_X.reshape(test_fs_X.shape[0], 1, test_fs_X.shape[1]), Y: to_categorical(label_encoder.transform(test_fs[0]))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "F1 Score for each model, conveys the balance between the precision and the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB(Sklearn) F1 Score:  0.9707093359216391 , Keras BI-RNN F1 Score:  0.9720171125440312\n",
      "INFO:tensorflow:Restoring parameters from ./model_tf/model_tf_adam\n",
      "TF BI-RNN F1 Score: 0.9705131168450581\n"
     ]
    }
   ],
   "source": [
    "# Custom Test File, Fellowship.AI\n",
    "\n",
    "test_fs = pd.read_csv('europarl.test', sep=\"\\t\", header=None)\n",
    "\n",
    "test_fs[1] = [clean_sentence(i, 0) for i in test_fs[1]]\n",
    "\n",
    "# Sklearn Evaluate\n",
    "mb_acc = f1_score(label_encoder.transform(test_fs[0]), mnb_clf.predict(count_vectorizer.transform(test_fs[1])), average='macro')\n",
    "\n",
    "# Keras Evaluate\n",
    "test_fs_X = count_vectorizer.transform(test_fs[1]).toarray()\n",
    "keras_acc = f1_score(label_encoder.transform(test_fs[0]), run_keras_birnn(None, None, model_path='/media/abhishek/55840FA23CCD9EE9/FellowshipAI_Challenge/keras_model_2epoch.h5').predict(test_fs_X.reshape(test_fs_X.shape[0], 1, test_fs_X.shape[1])).argmax(axis=-1), average='macro')\n",
    "\n",
    "print(\"MNB(Sklearn) F1 Score: \", mb_acc, \", Keras BI-RNN F1 Score: \", keras_acc)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model_tf/model_tf_adam\")\n",
    "    \n",
    "    y_pred = np.array(sess.run(tf.argmax(prediction, 1), feed_dict={X: test_fs_X.reshape(test_fs_X.shape[0], 1, test_fs_X.shape[1]), Y: to_categorical(label_encoder.transform(test_fs[0]))}))\n",
    "    \n",
    "    print(\"TF BI-RNN F1 Score:\", \\\n",
    "            f1_score(label_encoder.transform(test_fs[0]), y_pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
